{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bypass the \"too many requests\" error\n",
    "yf.data.YfData.user_agent_headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [ # ticker universe that contains the top market cap stocks\n",
    "    \"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"AMZN\", \"META\", \"TSLA\", \"AVGO\", \"BRK-B\", \"WMT\",\n",
    "    \"LLY\", \"JPM\", \"V\", \"MA\", \"UNH\", \"ORCL\", \"XOM\", \"TSM\", \"JNJ\", \"PG\",\n",
    "    \"HD\", \"CVX\", \"KO\", \"PEP\", \"BAC\", \"MRK\", \"ABBV\", \"PFE\", \"CSCO\", \"ADBE\",\n",
    "    \"NFLX\", \"NKE\", \"VZ\", \"INTC\", \"T\", \"CMCSA\", \"CRM\", \"ABT\", \"MCD\", \"DIS\",\n",
    "    \"BABA\", \"TM\", \"SHEL\", \"NVS\", \"ASML\", \"AZN\", \"SNY\", \"LIN\", \"PM\", \"ACN\",\n",
    "    \"TMO\", \"DHR\", \"COST\", \"WFC\", \"MS\", \"UPS\", \"BMY\", \"TXN\", \"NEE\", \"QCOM\",\n",
    "    \"AMGN\", \"UL\", \"SAP\", \"UNP\", \"MDT\", \"HON\", \"INTU\", \"LOW\", \"SCHW\",\n",
    "    \"RTX\", \"AMT\", \"SPGI\", \"GS\", \"BLK\", \"IBM\", \"PLD\", \"CVS\", \"DE\", \"SBUX\",\n",
    "    \"BA\", \"CAT\", \"ELV\", \"NOW\", \"PYPL\", \"SHOP\", \"SONY\", \"MUFG\", \"HDB\", \"TD\",\n",
    "    \"BP\", \"RIO\", \"OR\", \"ENB\", \"EQNR\", \"SREN.SW\", \"ZURN.SW\", \"ROG.SW\", \"GIVN.SW\",\n",
    "    \"ADEN.SW\", \"NESN.SW\", \"UHR.SW\", \"CFR.SW\", \"RACE\", \"MBG.DE\",\n",
    "    \"AIR.PA\", \"BAYN.DE\", \"BMW.DE\", \"DTE.DE\", \"DBK.DE\", \"SAN.PA\", \"OR.PA\",\n",
    "    \"MC.PA\", \"BNP.PA\", \"SU.PA\", \"DG.PA\", \"ENGI.PA\", \"VIV.PA\", \"BARC.L\",\n",
    "    \"LLOY.L\", \"GSK.L\", \"TSCO.L\", \"HSX.L\", \"VALE\", \"PBR\", \"BBDC4.SA\", \"ITUB4.SA\",\n",
    "    \"WEGE3.SA\", \"ABEV\", \"BRFS3.SA\", \"MELI\", \"YPF\", \"ORSTED.CO\",\n",
    "    \"NOVO-B.CO\", \"DSV.CO\", \"VWS.CO\", \"EQT\", \"DDOG\", \"SNOW\", \"PLTR\", \"TWLO\", \"SE\", \"BIDU\",\n",
    "    \"NTES\", \"JD\", \"PDD\", \"XPEV\", \"NIO\", \"LI\", \"STLA\", \"MTX.DE\",\n",
    "    \"SAP.DE\", \"VOW3.DE\", \"RWE.DE\", \"HEN3.DE\", \"IFX.DE\", \"BYDDY\", \"TCEHY\", \"NOK\",\n",
    "    \"SPY\", \"VT\", \"GLD\", \"XLF\", \"XLV\", \"XLE\", \"QQQ\", \"IONQ\", \"XPH\", \"FSLR\", \"ENPH\",\n",
    "    \"VWDRY\", \"CSIQ\", \"GXC\", \"CQQQ\", \"EL\", \"VERI\", \"ASTS\"\n",
    "]\n",
    "tickers = sorted(tickers)\n",
    "\n",
    "# Dictionary to store ticker-currency mappings\n",
    "ticker_currencies = {}\n",
    "scraped = {} # save the info for later in a dictionary to avoid asking yf again\n",
    "for ticker in tickers:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    scraped[ticker] = stock\n",
    "    currency = stock.info.get(\"currency\", None)  # Retrieve currency info\n",
    "    ticker_currencies[ticker] = currency.upper()\n",
    "    \n",
    "currency_set = set(ticker_currencies.values())\n",
    "currency_set.discard(\"CHF\")\n",
    "currency_tickers = [\"CHF\" + currency + \"=X\" for currency in currency_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tickers are international, so we have to be careful to convert prices to CHF before computing returns. We can grab the currencies straight from `yfinance`.\n",
    "\n",
    "The `data` variable stores the prices for every two days (we skip a day to reduce noise in the currency exchange and closing time differences between markets). Prices are adjusted to accound for dividend accrual. This means that they do not reflect the true historical price! The `returns` variable gives the percent change over 2 days. The currency exchanges are removed from `returns` and kept in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading historical data...\")\n",
    "n_years = 5\n",
    "end_date = datetime.datetime.today()\n",
    "start_date = end_date - datetime.timedelta(days=n_years * 365)\n",
    "\n",
    "# Download data for tickers and currency_tickers\n",
    "data = yf.download(tickers + currency_tickers, start=start_date, end=end_date, interval=\"1d\", progress=False)\n",
    "data = data[\"Close\"].dropna(how=\"all\").ffill().dropna()\n",
    "\n",
    "# Convert index to string format (YYYY-MM-DD)\n",
    "data.index = pd.to_datetime(data.index).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Exchange currency if necessary\n",
    "    currency = ticker_currencies[ticker]\n",
    "\n",
    "    # Get dividends\n",
    "    dividends = scraped[ticker].history(period=\"5y\")[\"Dividends\"]\n",
    "    dividends.index = pd.to_datetime(dividends.index).strftime(\"%Y-%m-%d\")\n",
    "    dividends = dividends.reindex(data.index, fill_value=0)\n",
    "    if currency == \"GBP\":\n",
    "        dividends /= 100 # weird way to record in Britain\n",
    "\n",
    "    data[ticker] += dividends.cumsum()\n",
    "    \n",
    "    if currency != \"CHF\":\n",
    "        data[ticker] /= data[\"CHF\" + currency + \"=X\"].values\n",
    "\n",
    "data = data.iloc[::-2].iloc[::-1]\n",
    "returns = np.log(data.drop(currency_tickers, axis=1)).diff().dropna()\n",
    "print(\"Got the historical data!\")\n",
    "returns.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the `market_caps` of each ticker and feed it through the covariance matrix to get the market's expected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_cap(ticker):\n",
    "    ticker_info = scraped[ticker].info\n",
    "    \n",
    "    # get marketCap\n",
    "    currency = ticker_currencies[ticker]\n",
    "    if currency == \"CHF\":\n",
    "        conversion = 1\n",
    "    else:\n",
    "        conversion = data[\"CHF\" + currency + \"=X\"].iloc[-1]\n",
    "    cap_keys = (\"marketCap\", \"totalAssets\")\n",
    "    for key in cap_keys:\n",
    "        if key in ticker_info.keys():\n",
    "            return ticker_info[key] / conversion\n",
    "            \n",
    "    return None\n",
    "\n",
    "print(\"Getting market cap data...\")\n",
    "market_caps = pd.Series([get_market_cap(ticker) for ticker in tickers], index=tickers)\n",
    "print(\"Got the market caps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance calculation\n",
    "\n",
    "Here, we use two techniques to calculate the covariance matrix.\n",
    "- We weight more recent observations in `returns` more than the older ones when computing the basic covariance matrix\n",
    "- Then, we use Ledoit-Wolf shrinkage to correct for overfitting. The target covariance matrix uses uniform correlation between all the stocks.\n",
    "\n",
    "*Note, since the returns contain dividend payments, the dividends are accounded for through the covariance matrix only*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.index = pd.to_datetime(returns.index)\n",
    "time_span = returns.index[-1] - returns.index[0]\n",
    "n_years = time_span.days / 365\n",
    "entries_per_year = len(returns) / n_years\n",
    "\n",
    "# Calculate the covariance matrix based on weights that decrease exponentially with time\n",
    "lamb = (1/n_years)**(1/n_years) # lambda in the exponential time weighting\n",
    "weights = lamb**(np.array([(end_date - day).days for day in returns.index]) / 365)\n",
    "weights /= np.sum(weights)\n",
    "weighted_mean = (returns * weights[:, None]).sum()\n",
    "weighted_cov = ((returns - weighted_mean) * weights[:, None]).T @ (returns - weighted_mean)\n",
    "weighted_cov *= entries_per_year\n",
    "\n",
    "# Estimate the correlation for Ledoit-Wolf shrinkage\n",
    "delta = 0.2 # larger --> more shrinkage\n",
    "standard_devs = np.sqrt(np.diag(weighted_cov))\n",
    "weighted_corr = weighted_cov / standard_devs[None, :] / standard_devs[:, None]\n",
    "average_corr = (np.sum(weighted_corr.values) - len(weighted_corr)) / len(weighted_corr) / (len(weighted_corr) - 1)\n",
    "target_cov = np.full(weighted_cov.shape, average_corr)\n",
    "np.fill_diagonal(target_cov, 1)\n",
    "target_cov = pd.DataFrame(target_cov, index=weighted_cov.index, columns=weighted_cov.index)\n",
    "target_cov = target_cov * standard_devs[None, :] * standard_devs[:, None]\n",
    "sigma = delta * target_cov + (1 - delta) * weighted_cov # this is the empirical Sigma in the BL formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returns calculation\n",
    "\n",
    "Our main tool will be the Black-Litterman model. Here, we assume that the market caps are weights used by the global economy, and we can calculate the expected returns by passing these weights through `sigma`.\n",
    "\n",
    "From these expected returns, we update them based on our `views`. Our views incorporate `self.forces` (which measure the predicted Z score of the portfolio price given by `self.P`)\n",
    "\n",
    "The final adjusted `expected_returns` will be used to optimize our portfolio by conditioning on a minimum total expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    yf.Ticker(\"^IRX\").history(period=\"1d\")\n",
    "    risk_free_rate = None\n",
    "except:\n",
    "    risk_free_rate = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market return\n",
    "risk_aversion = 3.5 # higher means that investors require higher returns for a single unit of risk\n",
    "pi = risk_aversion * sigma @ market_caps / market_caps.sum() # exess returns (after risk free + dividends)\n",
    "pi = pi.squeeze()\n",
    "oil_gas_tickers = [\"BP\", \"CVX\", \"ENB\", \"EQNR\", \"PBR\", \"RIO\", \"SHEL\", \"VALE\", \"XOM\", \"XLE\"]\n",
    "\n",
    "one_hot = pd.DataFrame(np.eye(len(pi)), columns=pi.index) # Creates a a one-hot vector corresponding to each ticker to be fed to Views\n",
    "\n",
    "class Views():\n",
    "    def __init__(self, risk_free_rate=None):\n",
    "        self.P = None\n",
    "        self.forces = None\n",
    "        self.expected_returns = None\n",
    "        if risk_free_rate is None:\n",
    "            # Fetch the 3-month Treasury Bill yield (^IRX)\n",
    "            rf_data = yf.Ticker(\"^IRX\").history(period=\"1d\")\n",
    "            self.risk_free_rate = rf_data[\"Close\"].iloc[-1] / 100 # Convert from a percent\n",
    "        else:\n",
    "            self.risk_free_rate = risk_free_rate\n",
    "\n",
    "    def add_view(self, view, force):\n",
    "        view = np.array(view)\n",
    "        view = view.reshape(1, -1)\n",
    "        if self.P is None:\n",
    "            self.P = view\n",
    "            self.forces = np.array(force, ndmin=1)\n",
    "        else:\n",
    "            self.P = np.vstack((self.P, view))\n",
    "            self.forces = np.append(self.forces, force)\n",
    "\n",
    "    def compute_returns(self, remove=None, verbose=False):\n",
    "        if self.P is None:\n",
    "            self.expected_returns = pi + self.risk_free_rate\n",
    "        else:\n",
    "            sigma_22 = self.P @ sigma.values @ self.P.T\n",
    "            eigvals, eigvecs = np.linalg.eigh(sigma_22)\n",
    "            assert np.all(eigvals > 0), \"sigma_22 is not positive definite!\"\n",
    "            sqrt_sigma22_inv = eigvecs @ np.diag(1.0 / np.sqrt(eigvals)) @ eigvecs.T\n",
    "            pi_adjusted = pi.values + (sigma.values @ self.P.T @ sqrt_sigma22_inv @ self.forces)\n",
    "\n",
    "            if verbose and self.P is not None:\n",
    "                print(\"Adjustments to the mean returns of the views:\")\n",
    "                print(eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T @ self.forces)\n",
    "\n",
    "            expected_returns = self.risk_free_rate + pd.Series(pi_adjusted, pi.index)\n",
    "            if remove is not None:\n",
    "                expected_returns.loc[remove] = 0\n",
    "            self.expected_returns = expected_returns\n",
    "        return self.expected_returns\n",
    "\n",
    "# Incorporate my views\n",
    "views = Views(risk_free_rate=risk_free_rate)\n",
    "views.add_view(one_hot[\"TSLA\"], -0.15)\n",
    "views.add_view(one_hot[\"META\"], -0.05)\n",
    "views.add_view(one_hot[\"GLD\"], -0.1)\n",
    "views.add_view((one_hot[\"NVDA\"] + one_hot[\"AVGO\"] + one_hot[\"TSM\"])/3, 0.1)\n",
    "views.add_view((2 * one_hot[\"IBM\"] + one_hot[\"IONQ\"])/3, 0.1) # Bullish on quantum, especially IBM\n",
    "views.add_view(one_hot[\"CQQQ\"] - one_hot[\"QQQ\"], 0.1) # Bullish on China's ability to innovate\n",
    "views.add_view(one_hot[\"NIO\"], 0.1)\n",
    "views.add_view(one_hot[\"CSIQ\"], 0.05)\n",
    "print(views.compute_returns(remove=oil_gas_tickers, verbose=True))\n",
    "\n",
    "market = Views(risk_free_rate=risk_free_rate)\n",
    "market.compute_returns(remove=oil_gas_tickers)\n",
    "\n",
    "boom = Views(risk_free_rate=0.05)\n",
    "boom.add_view(one_hot[\"SPY\"], 2)\n",
    "boom.compute_returns(remove=oil_gas_tickers, verbose=True)\n",
    "\n",
    "crash = Views(risk_free_rate=0)\n",
    "crash.add_view(one_hot[\"SPY\"], -2) # 2 standard deviations away from equilibrium\n",
    "crash.compute_returns(remove=oil_gas_tickers, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"SPY\"\n",
    "print(views.expected_returns[ticker].item())\n",
    "print(market.expected_returns[ticker].item())\n",
    "print(boom.expected_returns[ticker].item())\n",
    "print(crash.expected_returns[ticker].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate more data\n",
    "\n",
    "Our optimization strategy will involve calculating extreme losses. Due to the limited historical data, we simulate extra data by linearly transforming independent Student-T prices to have covariance `sigma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L = np.linalg.cholesky(sigma / entries_per_year)\n",
    "def simulate_year(views, df_student):\n",
    "    if views.expected_returns is None:\n",
    "        views.compute_returns(remove=oil_gas_tickers)\n",
    "    # Set parameters\n",
    "    sim_historic_ratio = entries_per_year # infinity\n",
    "    \n",
    "    n_total = int(entries_per_year)\n",
    "    n_sample = int(entries_per_year / (sim_historic_ratio + 1))\n",
    "    n_sim = n_total - n_sample\n",
    "\n",
    "    simulated_returns = stats.t.rvs(df=df_student, size=(n_sim, len(returns.columns)))  # Standard-t\n",
    "    simulated_returns /= np.sqrt(df_student / (df_student - 2)) # Normalize to unit variance\n",
    "    assert sigma.columns is returns.columns, \"Non-matching columns\"\n",
    "    simulated_returns = pd.DataFrame(simulated_returns @ L.T, columns=returns.columns)\n",
    "    \n",
    "    dates = np.random.choice(returns.index, size=n_sample, replace=True, p=weights)\n",
    "    return returns.loc[dates].sum(axis=0) + simulated_returns.sum(axis=0) + n_sim * views.expected_returns / entries_per_year\n",
    "\n",
    "\n",
    "years_per_scenario = 3000\n",
    "scenarios = [views, market, boom, crash]\n",
    "probabilities = np.array([4, 1, 1, 1])\n",
    "dfs = [5, 5, 4, 3]\n",
    "probabilities = probabilities / np.sum(probabilities)\n",
    "annual_returns = [pd.concat([simulate_year(scenario, df_student) for _ in range(years_per_scenario)], axis = 1).T for scenario, df_student in zip(scenarios, dfs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portfolio optimization\n",
    "\n",
    "The function `optimal_portfolio(...)` computes the portfolio weights that correspond to the minimum `desired_returns`, minimizing the vulnerability of the portfolio to extreme shocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scenarios = len(scenarios)\n",
    "n_assets = len(annual_returns[0].columns)\n",
    "\n",
    "def optimal_portfolio(desired_returns,\n",
    "                      verbose=False,\n",
    "                      partial_weights=None,\n",
    "                      alpha=0.95,\n",
    "                      min_weight=0):\n",
    "    \"\"\"\n",
    "    This function computes the weights of the portfolio that is optimized for the\n",
    "    global `scenarios` variable, with associated `probabilities` and `dfs`.\n",
    "\n",
    "    Parameters:\n",
    "        - desired_returns (float): The minimum expected returns of the portfolio across\n",
    "        all scenarios, weighted by the probabilities. If it is not possible to\n",
    "        cunstruct such a portfolio, the function throws an AssertionError.\n",
    "        - verbose (bool): Print additional information and plot the portfolio's history.\n",
    "        - partial_weights (pd.Series): The minimum portfolio weights in the resulting\n",
    "        portfolio. Requires partial_weights.sum() < 1.\n",
    "        - alpha (float): The quantile at which the value at risk (VaR) is computed.\n",
    "        - min_weight (float): Stocks that are assigned weights less than this value are\n",
    "        excluded from the portfolio.\n",
    "\n",
    "    Returns a dictionary containing:\n",
    "        - \"VaR\" : the value at risk at the spec,\n",
    "        - \"ES\" : a vector of expected shortfalls in each scenario,\n",
    "        - \"Quantile\" : alpha,\n",
    "        - \"Returns\" : the expected returns of the portfolio. This might be less than\n",
    "        desired_returns if min_weights > 0.\n",
    "        - \"Weights\" : The portfolio weights.\n",
    "        - \"Risk\" : The maximum weighted risk measure (expected shortfall weighted by\n",
    "        by probabilities) across all scenarios.\n",
    "    \"\"\"\n",
    "    \n",
    "    if partial_weights is None:\n",
    "        partial_weights = 0\n",
    "        partial_weight_amount = 0\n",
    "    else:\n",
    "        assert partial_weights.sum() < 1, \"Invalid partial_weights.\"\n",
    "        partial_weights = partial_weights.values\n",
    "        partial_weight_amount = np.sum(partial_weights)\n",
    "\n",
    "    # Define CVXPY variables.\n",
    "    w = cp.Variable(n_assets) # portfolio weights\n",
    "    nu = cp.reshape(cp.Variable(n_scenarios), (n_scenarios, 1), order='C') # VaR at level alpha (to be determined)\n",
    "    xi = cp.Variable((n_scenarios, years_per_scenario)) # auxiliary variables for the losses beyond VaR\n",
    "    losses = cp.vstack([- ret.values @ (w + partial_weights) for ret in annual_returns]) # one loss for each scenario\n",
    "    expected_returns = [scenario.compute_returns(remove=oil_gas_tickers) * probabilities[i] for i, scenario in enumerate(scenarios)]\n",
    "    expected_returns = pd.concat(expected_returns, axis=1).sum(axis=1) # already probability weighted\n",
    "    expected_shortfall = nu + (1/(1-alpha))*(1/years_per_scenario)*cp.reshape(cp.sum(xi, axis=1), (n_scenarios, 1), order='C')\n",
    "    weighted_risk_measures = cp.multiply(probabilities.reshape((n_scenarios, 1)), expected_shortfall)\n",
    "\n",
    "    max_weighted_risk_measure = cp.Variable()\n",
    "\n",
    "    # Constraints for CVaR:\n",
    "    constraints = [\n",
    "        xi >= losses - nu,                               # each xi >= loss_i - nu\n",
    "        xi >= 0,                                         # slack variables non-negative\n",
    "        cp.sum(w) == 1 - partial_weight_amount,          # full (complementary) investment\n",
    "        w >= 0,                                          # long-only constraint\n",
    "        # expected annual constraint\n",
    "        expected_returns.values @ (w + partial_weights) >= desired_returns,  \n",
    "        max_weighted_risk_measure >= weighted_risk_measures\n",
    "    ]\n",
    "\n",
    "    objective = cp.Minimize(max_weighted_risk_measure)\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Solve optimization\n",
    "    problem.solve(solver=cp.ECOS)\n",
    "    assert w.value is not None, \"No solution!\"\n",
    "    optimized_weights = pd.Series(w.value + partial_weights, index=expected_returns.index)\n",
    "    optimized_weights[optimized_weights < min_weight] = 0\n",
    "    optimized_weights /= optimized_weights.sum()\n",
    "    optimized_returns = np.dot(expected_returns.values, optimized_weights.values)\n",
    "\n",
    "    if(verbose):\n",
    "        print(f\"Quantile: {alpha:.3f}\")\n",
    "        print(f\"VaR:\")\n",
    "        print(nu.value)\n",
    "        print(f\"ES:\")\n",
    "        print(expected_shortfall.value)\n",
    "        print(f\"Total returns: {optimized_returns:.4f}\")\n",
    "        print(f\"Portfolio risk: {max_weighted_risk_measure.value:.4f}\")\n",
    "        print(\"---\")\n",
    "        print(optimized_weights[optimized_weights > 0])\n",
    "        portfolio_history = data[expected_returns.index].values @ optimized_weights.values\n",
    "        portfolio_history = pd.Series(portfolio_history, pd.to_datetime(data.index))\n",
    "        plt.plot(portfolio_history / portfolio_history.iloc[0])\n",
    "        plt.title(\"Portfolio history\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Cumulative returns\")\n",
    "    return({\"VaR\" : nu.value,\n",
    "            \"ES\" : expected_shortfall,\n",
    "            \"Quantile\" : alpha,\n",
    "            \"Returns\" : optimized_returns,\n",
    "            \"Weights\" : optimized_weights,\n",
    "            \"Risk\" : max_weighted_risk_measure})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired_returns = market.expected_returns[\"SPY\"]\n",
    "desired_returns = 0.12 # A little bit less than SPY\n",
    "# partial_weights = 0.3 * one_hot[\"NIO\"] + 0.1 * one_hot[\"NVDA\"] # just as an example\n",
    "partial_weights = None\n",
    "my_portfolio = optimal_portfolio(desired_returns, verbose=True, partial_weights=partial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "returns_seq = np.arange(0.06, 0.17, 0.01)\n",
    "risk_seq = [optimal_portfolio(r)[\"Risk\"].value for r in returns_seq]\n",
    "\n",
    "# Plot both lines\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(returns_seq, risk_seq, label=\"Risk\", linestyle='-', marker='o')\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Target Return\")\n",
    "plt.ylabel(f\"Maximum weighted portfolio risk\")\n",
    "plt.title(\"CVaR Optimization\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"SPY\"\n",
    "data.index = pd.to_datetime(data.index)\n",
    "plt.plot(data[ticker] / data[ticker].iloc[0])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative returns\")\n",
    "plt.title(ticker)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code to calculate the `risk aversion` parameter.\n",
    "\n",
    "```python\n",
    "def estimate_risk_aversion():\n",
    "    \"\"\"\n",
    "    Estimate the Black-Litterman risk aversion coefficient (lambda)\n",
    "    based on market conditions.\n",
    "    \n",
    "    Uses:\n",
    "    - VIX (volatility index)\n",
    "    - 10-year Treasury Yield (^TNX)\n",
    "    - S&P 500 drawdown (SPY)\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated risk aversion coefficient (lambda)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fetch market data\n",
    "    vix = yf.Ticker(\"^VIX\").history(period=\"1d\")[\"Close\"].iloc[-1]  # Market volatility\n",
    "    tnx = yf.Ticker(\"^TNX\").history(period=\"1d\")[\"Close\"].iloc[-1] / 100  # Convert from % to decimal\n",
    "    spy = yf.Ticker(\"SPY\").history(period=\"6mo\")[\"Close\"]  # S&P 500 ETF\n",
    "    \n",
    "    # Compute max drawdown over the last 6 months\n",
    "    max_price = spy.max()\n",
    "    current_price = spy.iloc[-1]\n",
    "    drawdown = (max_price - current_price) / max_price  # Drawdown as a fraction\n",
    "    \n",
    "    # Baseline risk aversion\n",
    "    lambda_base = 3.5  # Normal market\n",
    "    \n",
    "    # Adjust for VIX (higher VIX → higher lambda)\n",
    "    if vix < 15:  # Low volatility\n",
    "        lambda_vix = 3.0\n",
    "    elif 15 <= vix < 25:  # Normal volatility\n",
    "        lambda_vix = 3.5\n",
    "    elif 25 <= vix < 35:  # High volatility\n",
    "        lambda_vix = 4.5\n",
    "    else:  # Very high volatility (panic)\n",
    "        lambda_vix = 5.5\n",
    "    \n",
    "    # Adjust for interest rates (higher rates → higher lambda)\n",
    "    if tnx < 0.02:  # Very low rates\n",
    "        lambda_tnx = 3.0\n",
    "    elif 0.02 <= tnx < 0.04:  # Moderate rates\n",
    "        lambda_tnx = 3.5\n",
    "    else:  # High rates\n",
    "        lambda_tnx = 4.5\n",
    "    \n",
    "    # Adjust for drawdown (larger drawdown → higher lambda)\n",
    "    if drawdown < 0.05:  # No major downturn\n",
    "        lambda_dd = 3.0\n",
    "    elif 0.05 <= drawdown < 0.10:  # Mild downturn\n",
    "        lambda_dd = 4.0\n",
    "    elif 0.10 <= drawdown < 0.20:  # Market correction\n",
    "        lambda_dd = 5.0\n",
    "    else:  # Bear market\n",
    "        lambda_dd = 6.0\n",
    "\n",
    "    # Weighted average of the different risk aversion factors\n",
    "    lambda_final = np.mean([lambda_base, lambda_vix, lambda_tnx, lambda_dd])\n",
    "\n",
    "    print(f\"Estimated risk aversion coefficient (lambda): {lambda_final:.2f}\")\n",
    "    return lambda_final\n",
    "\n",
    "# Run the function to estimate lambda dynamically\n",
    "lambda_dynamic = estimate_risk_aversion()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
